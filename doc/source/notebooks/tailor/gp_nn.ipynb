{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mixing TensorFlow models with GPflow\n",
    "\n",
    "This notebook explores two ways to combine TensorFlow neural networks with GPflow models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-20T10:53:12.620472Z",
     "start_time": "2018-06-20T10:53:11.541346Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from matplotlib import pyplot as plt\n",
    "import gpflow\n",
    "from scipy.cluster.vq import kmeans2\n",
    "from gpflow import default_float\n",
    "from gpflow.base import TensorLike\n",
    "from gpflow.ci_utils import is_continuous_integration\n",
    "\n",
    "ITERATIONS = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1: A ConvNet inside a GPflow model\n",
    "Here we'll use the GPflow functionality, but put a non-GPflow model inside the kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-20T10:53:16.316761Z",
     "start_time": "2018-06-20T10:53:12.621686Z"
    }
   },
   "outputs": [],
   "source": [
    "mnist = tf.keras.datasets.mnist\n",
    "\n",
    "class Mnist:\n",
    "    input_dim = 784\n",
    "    Nclasses = 10\n",
    "    (X, Y), (Xtest, Ytest) = mnist.load_data()\n",
    "    Y = Y[:, None]\n",
    "    Ytest = Ytest[:, None]\n",
    "\n",
    "if is_continuous_integration():\n",
    "    mask = (Mnist.Y <= 1).squeeze()\n",
    "    Mnist.X = Mnist.X[mask][:105, 300:305]\n",
    "    Mnist.Y = Mnist.Y[mask][:105]\n",
    "    mask = (Mnist.Ytest <= 1).squeeze()\n",
    "    Mnist.Xtest = Mnist.Xtest[mask][:10, 300:305]\n",
    "    Mnist.Ytest = Mnist.Ytest[mask][:10]\n",
    "    Mnist.input_dim = 5\n",
    "    Mnist.Nclasses = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-20T10:53:16.335949Z",
     "start_time": "2018-06-20T10:53:16.317967Z"
    }
   },
   "outputs": [],
   "source": [
    "# A vanilla ConvNet\n",
    "# This gets 97.3% accuracy on MNIST when used on its own (+ final linear layer) after 20K iterations\n",
    "def cnn_fn(x, output_dim):\n",
    "    \"\"\"\n",
    "    Adapted from https://www.tensorflow.org/tutorials/layers\n",
    "    \"\"\"\n",
    "    conv1 = tf.layers.conv2d(\n",
    "          inputs=tf.reshape(x, [-1, 28, 28, 1]),\n",
    "          filters=32,\n",
    "          kernel_size=[5, 5],\n",
    "          padding=\"same\",\n",
    "          activation=tf.nn.relu)\n",
    "\n",
    "    pool1 = tf.layers.max_pooling2d(inputs=conv1, pool_size=[2, 2], strides=2)\n",
    "\n",
    "    conv2 = tf.layers.conv2d(\n",
    "          inputs=pool1,\n",
    "          filters=64,\n",
    "          kernel_size=[5, 5],\n",
    "          padding=\"same\",\n",
    "          activation=tf.nn.relu)\n",
    "    \n",
    "    pool2 = tf.layers.max_pooling2d(inputs=conv2, pool_size=[2, 2], strides=2)\n",
    "\n",
    "    pool2_flat = tf.reshape(pool2, [-1, 7 * 7 * 64])\n",
    "    return tf.layers.dense(inputs=pool2_flat, units=output_dim, activation=tf.nn.relu)\n",
    "\n",
    "if is_continuous_integration():\n",
    "    def cnn_fn(x, output_dim):\n",
    "        return tf.layers.dense(inputs=tf.reshape(x, [-1, Mnist.input_dim]), units=output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-20T10:53:16.475537Z",
     "start_time": "2018-06-20T10:53:16.337305Z"
    }
   },
   "outputs": [],
   "source": [
    "class KernelWithNN(gpflow.kernels.Kernel):\n",
    "    \"\"\"\n",
    "    This kernel class allows for easily adding a NN (or other function) to a GP model.\n",
    "    The kernel does not actually do anything with the NN.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, kern, f):\n",
    "        \"\"\"\n",
    "        kern.input_dim needs to be consistent with the output dimension of f\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.kern = kern\n",
    "        self._f = f\n",
    "        \n",
    "    def f(self, X):\n",
    "        if X is not None:\n",
    "            with tf.variable_scope('forward', reuse=tf.AUTO_REUSE):\n",
    "                return self._f(X)\n",
    "    \n",
    "    def _get_f_vars(self):\n",
    "        return tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='forward')\n",
    "\n",
    "    \n",
    "    def K(self, X, X2=None):\n",
    "        return self.kern(X, X2)\n",
    "    \n",
    "    def Kdiag(self, X):\n",
    "        return self.kern(X, full_cov=False)\n",
    "\n",
    "\n",
    "class KernelSpaceInducingPoints(gpflow.inducing_variables.InducingPoints):\n",
    "    pass\n",
    "\n",
    "# Same Kuu as regular inducing points\n",
    "gpflow.covariances.Kuu.register(KernelSpaceInducingPoints, KernelWithNN)(\n",
    "    gpflow.covariances.Kuu.dispatch(gpflow.inducing_variables.InducingPoints, gpflow.kernels.Kernel)\n",
    ")\n",
    "\n",
    "# Kuf is in NN output space\n",
    "@gpflow.covariances.Kuf.register(KernelSpaceInducingPoints, KernelWithNN, TensorLike)\n",
    "def Kuf(inducing_variable: InducingPoints, kernel, Xnew):\n",
    "    return kernel(inducing_variable.Z, kernel.f(Xnew))\n",
    "\n",
    "class NNComposedKernel(KernelWithNN):\n",
    "    \"\"\"\n",
    "    This kernel class applies f() to X before calculating K\n",
    "    \"\"\"\n",
    "    \n",
    "    def K(self, X, X2=None):\n",
    "        return super().K(self.f(X), self.f(X2))\n",
    "    \n",
    "    def Kdiag(self, X):\n",
    "        return super().Kdiag(self.f(X))\n",
    "    \n",
    "# We need to add these extra functions to the model so the TensorFlow variables get picked up\n",
    "class NN_SVGP(gpflow.models.SVGP):\n",
    "    @property\n",
    "    def trainable_tensors(self):\n",
    "        return super().trainable_tensors + self.kern._get_f_vars()\n",
    "\n",
    "    @property\n",
    "    def initializables(self):\n",
    "        return super().initializables + self.kern._get_f_vars()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-20T11:10:46.858524Z",
     "start_time": "2018-06-20T10:53:16.477021Z"
    }
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input of rank > 2 is not supported.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-63-2c65751b4ab5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Accuracy is {:.4f}%'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0macc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0mex1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-63-2c65751b4ab5>\u001b[0m in \u001b[0;36mex1\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mlik\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgpflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlikelihoods\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMultiClass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMnist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNclasses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mZ\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkmeans2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMnist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mminit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'points'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNN_SVGP\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMnist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMnist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlik\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mZ\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mZ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_latent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mMnist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNclasses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mminibatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/gpflowopt_env/lib/python3.6/site-packages/scipy/cluster/vq.py\u001b[0m in \u001b[0;36mkmeans2\u001b[0;34m(data, k, iter, thresh, minit, missing, check_finite)\u001b[0m\n\u001b[1;32m    672\u001b[0m         \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    673\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 674\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Input of rank > 2 is not supported.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    675\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    676\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Input of rank > 2 is not supported."
     ]
    }
   ],
   "source": [
    "def ex1():\n",
    "    fX_dim = 5  \n",
    "    M = 100\n",
    "\n",
    "    # Unfortunately, only float32 and lower are supported by the Conv layers \n",
    "    f = lambda x: tf.cast(cnn_fn(tf.cast(x, tf.float32), fX_dim), float_type)\n",
    "    kern = NNComposedKernel(gpflow.kernels.Matern32(), f)\n",
    "\n",
    "    # Build the model \n",
    "\n",
    "    lik = gpflow.likelihoods.MultiClass(Mnist.Nclasses)\n",
    "\n",
    "    Z = kmeans2(Mnist.X, M, minit='points')[0]\n",
    "\n",
    "    model = NN_SVGP(Mnist.X, Mnist.Y, kern, lik, Z=Z, num_latent=Mnist.Nclasses, minibatch_size=1000)\n",
    "\n",
    "    \n",
    "    # Use GPflow wrappers to train. Note all session handling is done for us\n",
    "    gpflow.training.AdamOptimizer(0.001).minimize(model, maxiter=ITERATIONS)\n",
    "\n",
    "    # Predictions\n",
    "    m, v = model.predict_y(Mnist.Xtest)\n",
    "    preds = np.argmax(m, 1).reshape(Mnist.Ytest.shape)\n",
    "    correct = preds == Mnist.Ytest.astype(int)\n",
    "    acc = np.average(correct.astype(float)) * 100.\n",
    "\n",
    "    print('Accuracy is {:.4f}%'.format(acc))\n",
    "\n",
    "ex1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-20T11:25:46.165332Z",
     "start_time": "2018-06-20T11:10:46.860361Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'notebook_niter' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-64-05ab63f0deaa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Accuracy is {:.4f}%'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0macc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m \u001b[0mex2b\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-64-05ab63f0deaa>\u001b[0m in \u001b[0;36mex2b\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mex2b\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mfX_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mminibatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnotebook_niter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_n\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mM\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnotebook_niter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_n\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'notebook_niter' is not defined"
     ]
    }
   ],
   "source": [
    "def ex2b():\n",
    "    fX_dim = 5  \n",
    "    minibatch_size = notebook_niter(1000, test_n=10)\n",
    "    M = notebook_niter(100, test_n=5)\n",
    "\n",
    "    # Unfortunately, only float32 and lower are supported by the Conv layers \n",
    "    f = lambda x: tf.cast(cnn_fn(tf.cast(x, tf.float32), fX_dim), float_type)\n",
    "    kern = KernelWithNN(gpflow.kernels.Matern32(fX_dim), f)\n",
    "    \n",
    "    # Reset inducing (these live in a different space to X, so we need to be careful here)\n",
    "    ind = np.random.choice(Mnist.X.shape[0], minibatch_size, replace=False)\n",
    "    \n",
    "    # Currently a hack is needed due to model initialisation\n",
    "    feat = KernelSpaceInducingPoints(np.empty((M, fX_dim)))\n",
    "    #feat = FFeature(Z_0)  # Ideally, we could move the calculation of Z_0\n",
    "    \n",
    "    # Build the model \n",
    "\n",
    "    lik = gpflow.likelihoods.MultiClass(Mnist.Nclasses)\n",
    "\n",
    "    #Z = kmeans2(Mnist.X, M, minit='points')[0]\n",
    "\n",
    "    model = NN_SVGP(Mnist.X, Mnist.Y, kern, lik, feat=feat, num_latent=Mnist.Nclasses, minibatch_size=minibatch_size)\n",
    "\n",
    "    fZ = model.kern.compute_f(Mnist.X[ind])\n",
    "    # Z_0 = kmeans2(fZ, M)[0] might fail\n",
    "    Z_0 = fZ[np.random.choice(len(fZ), M, replace=False)]\n",
    "    model.feature.Z = Z_0\n",
    "\n",
    "    # Use GPflow wrappers to train. Note all session handling is done for us\n",
    "    gpflow.training.AdamOptimizer(0.001).minimize(model, maxiter=ITERATIONS)\n",
    "\n",
    "    # Predictions\n",
    "    m, v = model.predict_y(Mnist.Xtest)\n",
    "    preds = np.argmax(m, 1).reshape(Mnist.Ytest.shape)\n",
    "    correct = preds == Mnist.Ytest.astype(int)\n",
    "    acc = np.average(correct.astype(float)) * 100.\n",
    "\n",
    "    print('Accuracy is {:.4f}%'.format(acc))\n",
    "\n",
    "ex2b()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2: A GPflow model on top of a TensorFlow model\n",
    "Now we'll do things the other way; take a model implemented in TensorFlow and explain how to put a GPflow model on top."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-20T11:40:21.347439Z",
     "start_time": "2018-06-20T11:25:46.167693Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'notebook_niter' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-65-9e55d982a19e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m \u001b[0mex2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-65-9e55d982a19e>\u001b[0m in \u001b[0;36mex2\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mex2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mminibatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnotebook_niter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_n\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mgp_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mM\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnotebook_niter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_n\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'notebook_niter' is not defined"
     ]
    }
   ],
   "source": [
    "def ex2():\n",
    "    minibatch_size = notebook_niter(1000, test_n=10)\n",
    "    gp_dim = 5\n",
    "    M = notebook_niter(100, test_n=5)\n",
    "\n",
    "    # Placeholders\n",
    "    X = tf.placeholder(tf.float32, [minibatch_size, Mnist.input_dim])  # Fixed shape so num_data works in SVGP\n",
    "    Y = tf.placeholder(tf.float32, [minibatch_size, 1])\n",
    "    Xtest = tf.placeholder(tf.float32, [None, Mnist.input_dim])\n",
    "\n",
    "    # Build graph\n",
    "\n",
    "    with tf.variable_scope('cnn'):\n",
    "        f_X = tf.cast(cnn_fn(X, gp_dim), dtype=float_type)\n",
    "\n",
    "    with tf.variable_scope('cnn', reuse=True):\n",
    "        f_Xtest = tf.cast(cnn_fn(Xtest, gp_dim), dtype=float_type)\n",
    "\n",
    "    gp_model = gpflow.models.SVGP(f_X, tf.cast(Y, dtype=float_type), \n",
    "                                  gpflow.kernels.RBF(gp_dim), gpflow.likelihoods.MultiClass(Mnist.Nclasses), \n",
    "                                  Z=np.zeros((M, gp_dim)), # we'll set this later\n",
    "                                  num_latent=Mnist.Nclasses)\n",
    "\n",
    "    loss = -gp_model.likelihood_tensor\n",
    "\n",
    "    m, v = gp_model._build_predict(f_Xtest)\n",
    "    my, yv = gp_model.likelihood.predict_mean_and_var(m, v)\n",
    "\n",
    "    with tf.variable_scope('adam'):\n",
    "        opt_step = tf.train.AdamOptimizer(0.001).minimize(loss)\n",
    "\n",
    "    tf_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='adam')\n",
    "    tf_vars += tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='cnn')\n",
    "\n",
    "    # Initialise\n",
    "    sess = tf.Session()\n",
    "    sess.run(tf.variables_initializer(var_list=tf_vars))\n",
    "    gp_model.initialize(session=sess)\n",
    "    \n",
    "    # Reset inducing (these live in a different space to X, so we need to be careful here)\n",
    "    ind = np.random.choice(Mnist.X.shape[0], minibatch_size, replace=False)\n",
    "\n",
    "    fZ = sess.run(f_X, feed_dict={X:Mnist.X[ind]})\n",
    "    # Z_0 = kmeans2(fZ, M)[0] might fail\n",
    "    Z_0 = fZ[np.random.choice(len(fZ), M, replace=False)]\n",
    "\n",
    "    def set_gp_param(param, value):\n",
    "        sess.run(tf.assign(param.unconstrained_tensor, param.transform.backward(value)))\n",
    "\n",
    "    set_gp_param(gp_model.feature.Z, Z_0)\n",
    "\n",
    "    # Train\n",
    "    for i in range(ITERATIONS):\n",
    "        ind = np.random.choice(Mnist.X.shape[0], minibatch_size, replace=False)\n",
    "        sess.run(opt_step, feed_dict={X:Mnist.X[ind], Y:Mnist.Y[ind]})\n",
    "\n",
    "    # Predict\n",
    "    preds = np.argmax(sess.run(my, feed_dict={Xtest:Mnist.Xtest}), 1).reshape(Mnist.Ytest.shape)\n",
    "    correct = preds == Mnist.Ytest.astype(int)\n",
    "    acc = np.average(correct.astype(float)) * 100.\n",
    "    print('acc is {:.4f}'.format(acc))\n",
    "\n",
    "\n",
    "ex2()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
